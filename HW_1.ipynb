{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8fefbf",
   "metadata": {},
   "source": [
    "# Homework 1 (include all parts from challenge 1+optional, 2 and homework)\n",
    "Authors:\n",
    "- Nazarii Drushchak\n",
    "- Igor Babin\n",
    "- Uliana Zbezhkhovska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8db555-03c4-49cc-84be-497d6dc4f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install -q annoy\n",
    "!pip install -q joblib\n",
    "!pip install -q joblibspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32012033-9003-4c00-96a6-006006f4d387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:03:40.396127Z",
     "start_time": "2023-10-20T07:03:40.144436Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db68f88c-6305-4e69-9e04-d483bb43a9e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:23:49.571116Z",
     "start_time": "2023-10-20T08:23:49.570442Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.sql.functions import col, avg, when\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from annoy import AnnoyIndex\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from joblib import Parallel\n",
    "from joblibspark import register_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea159999-9445-4d0c-a2a9-8eb7eb700255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:21:37.892421Z",
     "start_time": "2023-10-20T08:21:37.820539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4afbb5ea098d:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff88f1b890>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext('local[*]')\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49b5ac-b515-40f3-91a2-75180e325cb2",
   "metadata": {},
   "source": [
    "## Challenge I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3ecb16-2896-4a6a-9986-ca121098f0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:21:45.513585Z",
     "start_time": "2023-10-20T08:21:44.029081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-09 06:34:52--  http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2023-09-03/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 16.182.72.173, 52.217.72.19, 16.182.105.165, ...\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|16.182.72.173|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1698431 (1.6M) [application/csv]\n",
      "Saving to: ‘listings.csv’\n",
      "\n",
      "listings.csv        100%[===================>]   1.62M  1.79MB/s    in 0.9s    \n",
      "\n",
      "2023-11-09 06:34:53 (1.79 MB/s) - ‘listings.csv’ saved [1698431/1698431]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2023-09-03/visualisations/listings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22130b8f-06df-436b-8db4-f8383622509b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:21:52.901663Z",
     "start_time": "2023-10-20T08:21:52.665613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+\n",
      "|    id|                name|host_id|host_name|neighbourhood_group|neighbourhood|latitude|longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|             license|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+\n",
      "|761411|Condo in Amsterda...|4013546|   Xsjong|               NULL|   Noord-Oost|52.40164|  4.95106|   Private room|   61|             3|              303| 2023-08-19|             2.30|                             2|             272|                   26|0363 D4AD DCF3 E7...|\n",
      "|768274|Rental unit in Am...|3678216|    J & R|               NULL|   Westerpark|52.38855|  4.88521|Entire home/apt|  327|             3|               78| 2023-08-16|             0.64|                             1|              16|                    6|0363 7A50 18E7 51...|\n",
      "|768737|Boat in Amsterdam...|3877342|   Nicole|               NULL|   Westerpark|52.37824|  4.86826|   Private room|  109|             2|              341| 2023-08-24|             2.73|                             3|              29|                   43|036396BE30827DDB9575|\n",
      "|771217|Houseboat in Amst...|4068486| Danielle|               NULL|         Zuid|52.34091|  4.84802|Entire home/apt|  290|             3|               10| 2019-01-02|             0.11|                             1|               0|                    0|0363 D807 AD6C 49...|\n",
      "|771343|Rental unit in Am...|2313061|   Marcel|               NULL| Centrum-West|52.37641|  4.88303|   Private room|  150|             1|              699| 2023-09-01|             6.62|                             2|             196|                  110|0363 8C61 E9B9 55...|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"listings.csv\", header=True, multiLine=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cdfa5a7e14a33a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tokenize (remove punctuation and split by word), you can do it in pure python or using ml-lib tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5e9808-b943-46a6-ad12-97102173f0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:23:12.031476Z",
     "start_time": "2023-10-20T08:23:11.929531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+\n",
      "|    id|                name|host_id|host_name|neighbourhood_group|neighbourhood|latitude|longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|             license|               words|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+\n",
      "|761411|Condo in Amsterda...|4013546|   Xsjong|               NULL|   Noord-Oost|52.40164|  4.95106|   Private room|   61|             3|              303| 2023-08-19|             2.30|                             2|             272|                   26|0363 D4AD DCF3 E7...|[condo, in, amste...|\n",
      "|768274|Rental unit in Am...|3678216|    J & R|               NULL|   Westerpark|52.38855|  4.88521|Entire home/apt|  327|             3|               78| 2023-08-16|             0.64|                             1|              16|                    6|0363 7A50 18E7 51...|[rental, unit, in...|\n",
      "|768737|Boat in Amsterdam...|3877342|   Nicole|               NULL|   Westerpark|52.37824|  4.86826|   Private room|  109|             2|              341| 2023-08-24|             2.73|                             3|              29|                   43|036396BE30827DDB9575|[boat, in, amster...|\n",
      "|771217|Houseboat in Amst...|4068486| Danielle|               NULL|         Zuid|52.34091|  4.84802|Entire home/apt|  290|             3|               10| 2019-01-02|             0.11|                             1|               0|                    0|0363 D807 AD6C 49...|[houseboat, in, a...|\n",
      "|771343|Rental unit in Am...|2313061|   Marcel|               NULL| Centrum-West|52.37641|  4.88303|   Private room|  150|             1|              699| 2023-09-01|             6.62|                             2|             196|                  110|0363 8C61 E9B9 55...|[rental, unit, in...|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"name\", outputCol=\"words\")\n",
    "wordData = tokenizer.transform(df)\n",
    "wordData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef81fe3a240a51",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remove stopwords using ML-LIB stopwordsremover, and store in a new column called “CleanTokens”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf87163b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:23:14.536838Z",
     "start_time": "2023-10-20T08:23:14.385962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+\n",
      "|    id|                name|host_id|host_name|neighbourhood_group|neighbourhood|latitude|longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|             license|               words|         CleanTokens|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+\n",
      "|761411|Condo in Amsterda...|4013546|   Xsjong|               NULL|   Noord-Oost|52.40164|  4.95106|   Private room|   61|             3|              303| 2023-08-19|             2.30|                             2|             272|                   26|0363 D4AD DCF3 E7...|[condo, in, amste...|[condo, amsterdam...|\n",
      "|768274|Rental unit in Am...|3678216|    J & R|               NULL|   Westerpark|52.38855|  4.88521|Entire home/apt|  327|             3|               78| 2023-08-16|             0.64|                             1|              16|                    6|0363 7A50 18E7 51...|[rental, unit, in...|[rental, unit, am...|\n",
      "|768737|Boat in Amsterdam...|3877342|   Nicole|               NULL|   Westerpark|52.37824|  4.86826|   Private room|  109|             2|              341| 2023-08-24|             2.73|                             3|              29|                   43|036396BE30827DDB9575|[boat, in, amster...|[boat, amsterdam,...|\n",
      "|771217|Houseboat in Amst...|4068486| Danielle|               NULL|         Zuid|52.34091|  4.84802|Entire home/apt|  290|             3|               10| 2019-01-02|             0.11|                             1|               0|                    0|0363 D807 AD6C 49...|[houseboat, in, a...|[houseboat, amste...|\n",
      "|771343|Rental unit in Am...|2313061|   Marcel|               NULL| Centrum-West|52.37641|  4.88303|   Private room|  150|             1|              699| 2023-09-01|             6.62|                             2|             196|                  110|0363 8C61 E9B9 55...|[rental, unit, in...|[rental, unit, am...|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"CleanTokens\")\n",
    "cleanData = remover.transform(wordData)\n",
    "cleanData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07730f8022c5fef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But we don’t have a stopwordsremover for all language and contexts. Create your own list of stopwords from this text (think: what is a stopword?) Remove stopwords again, and store in column “MyCleanTokens”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79088e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:29:15.708713Z",
     "start_time": "2023-10-20T08:29:15.406575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|                name|host_id|host_name|neighbourhood_group|neighbourhood|latitude|longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|             license|               words|         CleanTokens|       MyCleanTokens|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|761411|Condo in Amsterda...|4013546|   Xsjong|               NULL|   Noord-Oost|52.40164|  4.95106|   Private room|   61|             3|              303| 2023-08-19|             2.30|                             2|             272|                   26|0363 D4AD DCF3 E7...|[condo, in, amste...|[condo, amsterdam...|[condo, amsterdam...|\n",
      "|768274|Rental unit in Am...|3678216|    J & R|               NULL|   Westerpark|52.38855|  4.88521|Entire home/apt|  327|             3|               78| 2023-08-16|             0.64|                             1|              16|                    6|0363 7A50 18E7 51...|[rental, unit, in...|[rental, unit, am...|[rental, unit, am...|\n",
      "|768737|Boat in Amsterdam...|3877342|   Nicole|               NULL|   Westerpark|52.37824|  4.86826|   Private room|  109|             2|              341| 2023-08-24|             2.73|                             3|              29|                   43|036396BE30827DDB9575|[boat, in, amster...|[boat, amsterdam,...|[boat, amsterdam,...|\n",
      "|771217|Houseboat in Amst...|4068486| Danielle|               NULL|         Zuid|52.34091|  4.84802|Entire home/apt|  290|             3|               10| 2019-01-02|             0.11|                             1|               0|                    0|0363 D807 AD6C 49...|[houseboat, in, a...|[houseboat, amste...|[houseboat, amste...|\n",
      "|771343|Rental unit in Am...|2313061|   Marcel|               NULL| Centrum-West|52.37641|  4.88303|   Private room|  150|             1|              699| 2023-09-01|             6.62|                             2|             196|                  110|0363 8C61 E9B9 55...|[rental, unit, in...|[rental, unit, am...|[rental, unit, am...|\n",
      "+------+--------------------+-------+---------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_list = ['the', 'a', 'an', 'another', \"for\", \"an\", \"nor\", \"but\", \"or\", \"yet\", \"so\", \n",
    "                                      \"in\", \"under\", \"towards\", \"before\"]\n",
    "remover = StopWordsRemover(stopWords=stop_list, inputCol='words', outputCol='MyCleanTokens')\n",
    "cleanData = remover.transform(cleanData)\n",
    "cleanData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674bb82c3d6416fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Perform TFIDF in a new column called “VectorSpace”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba62a4d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:24:40.550139Z",
     "start_time": "2023-10-20T08:24:40.150720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       MyCleanTokens|            features|\n",
      "+--------------------+--------------------+\n",
      "|[condo, amsterdam...|(20,[1,2,7,10,11,...|\n",
      "|[rental, unit, am...|(20,[1,2,5,11,12,...|\n",
      "|[boat, amsterdam,...|(20,[0,1,2,5,11,1...|\n",
      "|[houseboat, amste...|(20,[0,1,5,6,11,1...|\n",
      "|[rental, unit, am...|(20,[1,2,9,11,12,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"MyCleanTokens\", outputCol=\"VectorSpace\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(cleanData)\n",
    "\n",
    "idf = IDF(inputCol=\"VectorSpace\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "results = idfModel.transform(featurizedData)\n",
    "\n",
    "results.select(\"MyCleanTokens\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31413af3",
   "metadata": {},
   "source": [
    "## Homework (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1ffc194d6d238",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In a new column(‘word2vec’), repeat the procedure using word2vec instead of TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79972b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:25:02.165384Z",
     "start_time": "2023-10-20T08:25:01.417880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       MyCleanTokens|            word2vec|\n",
      "+--------------------+--------------------+\n",
      "|[condo, amsterdam...|[0.12229608697816...|\n",
      "|[rental, unit, am...|[0.15610642857583...|\n",
      "|[boat, amsterdam,...|[-0.0039486894384...|\n",
      "|[houseboat, amste...|[-0.1444090648482...|\n",
      "|[rental, unit, am...|[0.22863571302344...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2Vec = Word2Vec(vectorSize=20, minCount=0, inputCol=\"MyCleanTokens\", outputCol=\"word2vec\")\n",
    "model = word2Vec.fit(results)\n",
    "result = model.transform(results)\n",
    "\n",
    "result.select(\"MyCleanTokens\", \"word2vec\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3ef727b8a4830",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Show first row word2vec vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77f3c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:25:05.128521Z",
     "start_time": "2023-10-20T08:25:05.035056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(word2vec=DenseVector([0.1223, 0.2137, 0.2644, -0.0106, 0.4746, 0.1475, -0.0027, 0.4228, 0.0387, -0.115, 0.3691, -0.1899, -0.1823, 0.278, -0.0575, -0.332, 0.2215, -0.4446, 0.3223, 0.1145]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(\"word2vec\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31bd2a",
   "metadata": {},
   "source": [
    "## Challenge II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c6b7c23e348fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Take the first 500 flats in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49bba636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:25:11.667025Z",
     "start_time": "2023-10-20T08:25:11.553958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysample = result.limit(500)\n",
    "mysample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee23fade3c14e2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Find the 3 nearest neighbors for each element in that subset (candidates and query points are within the sample of 500) USING KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc90bd14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:25:25.052643Z",
     "start_time": "2023-10-20T08:25:24.792974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id ['761411', '634170', '721291', '730916']\n"
     ]
    }
   ],
   "source": [
    "mysample_pd = mysample.toPandas()\n",
    "tfidf = mysample_pd['features'].tolist()\n",
    "text = mysample_pd['name'].tolist()\n",
    "id_ = mysample_pd['id'].tolist()\n",
    "\n",
    "# fit nearest neighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=4).fit(tfidf)\n",
    "distances, indices = nbrs.kneighbors(tfidf[:5])\n",
    "\n",
    "# show 3 nearest neighbors for first row except itself\n",
    "print('id', [id_[i] for i in indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ebe3413ffdf4f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Find the 3 nearest neighbors for each element in that subset (candidates and query points are within the sample of 500) USING LSH with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a759685f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:27:28.528327Z",
     "start_time": "2023-10-20T08:27:28.486075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSHForest could not be imported\n"
     ]
    }
   ],
   "source": [
    "# IT IS DEPRECATED or Install 3 years old version of sklearn 0.16.1\n",
    "try:\n",
    "    from sklearn.neighbors import LSHForest\n",
    "    \n",
    "    mysample_pd = mysample.toPandas()\n",
    "    tfidf = mysample_pd['features'].tolist()\n",
    "    text = mysample_pd['name'].tolist()\n",
    "    id_ = mysample_pd['id'].tolist()\n",
    "    \n",
    "    lshf = LSHForest(random_state=42)\n",
    "    lshf.fit(tfidf)\n",
    "    \n",
    "    # get the feature vectore of the first row\n",
    "    query = tfidf[0]\n",
    "    id_ = id_[0]\n",
    "    \n",
    "    # show 3 nearest neighbors for first row except itself\n",
    "    distances, indices = lshf.kneighbors([query], n_neighbors=4)\n",
    "    for i in range(1, len(distances[0])):\n",
    "        print(\"distance: \", distances[0][i], \"id: \", id_[indices[0][i]])\n",
    "except ImportError:\n",
    "    print(\"LSHForest could not be imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d86be2c0e2ce48",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Find the 3 nearest neighbors for each element in that subset (candidates and query points are within the sample of 500) USING LSH with pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb01e17c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:27:44.212585Z",
     "start_time": "2023-10-20T08:27:43.738409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------+----------------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|     id|                name|host_id|       host_name|neighbourhood_group|neighbourhood|latitude|longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|             license|               words|         CleanTokens|       MyCleanTokens|         VectorSpace|            features|            word2vec|              hashes|           distCol|\n",
      "+-------+--------------------+-------+----------------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|1447217|Condo in Amsterda...|7767867|Martijn & Betina|               NULL|Bos en Lommer|52.37591|   4.8479|   Private room|  110|             7|              290| 2023-08-06|             2.40|                             1|              62|                   19|0363 D931 EAD5 9D...|[condo, in, amste...|[condo, amsterdam...|[condo, amsterdam...|(20,[1,2,7,10,11,...|(20,[1,2,7,10,11,...|[0.11496684015063...|[[4.7759737E8], [...|0.1428571428571429|\n",
      "|2175600|Loft in Amsterdam...|6630741|    Marie-Sophie|               NULL|         Zuid|52.35469|  4.85663|Entire home/apt|  190|             1|              115| 2023-07-09|             1.02|                             1|             233|                    6|0363 593F 238D 56...|[loft, in, amster...|[loft, amsterdam,...|[loft, amsterdam,...|(20,[1,2,7,10,11,...|(20,[1,2,7,10,11,...|[0.13099658095206...|[[4.7759737E8], [...|0.1428571428571429|\n",
      "| 721291|Condo in Amsterda...|3723928|        Katarina|               NULL|   Westerpark|52.37752|  4.87086|   Private room|   79|             2|              209| 2023-08-12|             1.58|                             1|             165|                   10|0363 0457 C25F CA...|[condo, in, amste...|[condo, amsterdam...|[condo, amsterdam...|(20,[1,2,7,10,11,...|(20,[1,2,7,10,11,...|[0.07579618220084...|[[2.0729985E7], [...|              0.25|\n",
      "+-------+--------------------+-------+----------------+-------------------+-------------+--------+---------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=3)\n",
    "model = mh.fit(mysample)\n",
    "\n",
    "# get the feature vector of the first row\n",
    "key =  mysample.select(\"features\").take(1)[0].features\n",
    "id_ = mysample.select(\"id\").take(1)[0].id\n",
    "\n",
    "\n",
    "# show 3 nearest neighbors for first row except itself\n",
    "model.approxNearestNeighbors(mysample, key, 4).filter(col(\"id\") != id_).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36b16b",
   "metadata": {},
   "source": [
    "## Challenge III: Homework\n",
    "\n",
    "![image.png](HW1_task.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2e8b670d448cf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Barcelona Airbnb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce81019dc53e84ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T07:28:45.296006Z",
     "start_time": "2023-10-20T07:28:43.290347Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-09 06:35:03--  http://data.insideairbnb.com/spain/catalonia/barcelona/2023-09-06/visualisations/listings.csv\n",
      "Resolving data.insideairbnb.com (data.insideairbnb.com)... 16.182.72.173, 52.217.72.19, 16.182.105.165, ...\n",
      "Connecting to data.insideairbnb.com (data.insideairbnb.com)|16.182.72.173|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3664972 (3.5M) [application/csv]\n",
      "Saving to: ‘listings_barcelona.csv’\n",
      "\n",
      "listings_barcelona. 100%[===================>]   3.50M  2.25MB/s    in 1.6s    \n",
      "\n",
      "2023-11-09 06:35:05 (2.25 MB/s) - ‘listings_barcelona.csv’ saved [3664972/3664972]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://data.insideairbnb.com/spain/catalonia/barcelona/2023-09-06/visualisations/listings.csv -O listings_barcelona.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "907808dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:28:09.594431Z",
     "start_time": "2023-10-20T08:28:09.396075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------+----------------+-------------------+--------------------+-----------------+-----------------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+\n",
      "|    id|                name|host_id|       host_name|neighbourhood_group|       neighbourhood|         latitude|        longitude|      room_type|price|minimum_nights|number_of_reviews|last_review|reviews_per_month|calculated_host_listings_count|availability_365|number_of_reviews_ltm|    license|\n",
      "+------+--------------------+-------+----------------+-------------------+--------------------+-----------------+-----------------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+\n",
      "| 18674|Rental unit in Ba...|  71615|Mireia And Maria|           Eixample|  la Sagrada Família|         41.40556|          2.17262|Entire home/apt|  202|             1|               38| 2023-06-26|             0.30|                            30|              34|                    8|HUTB-002062|\n",
      "| 23197|Rental unit in Sa...|  90417|  Etain (Marnie)|         Sant Martí|el Besòs i el Mar...|41.41243172529066|2.219750335269476|Entire home/apt|  255|             3|               73| 2023-08-15|             0.48|                             2|             150|                   11| HUTB005057|\n",
      "| 32711|Rental unit in Ba...| 135703|            Nick|             Gràcia|el Camp d'en Gras...|         41.40566|          2.17015|Entire home/apt|  171|            21|               95| 2023-08-18|             0.64|                             3|             310|                   21|HUTB-001722|\n",
      "|171646|Rental unit in Ba...| 400154|          Mireia|         Sant Martí|             el Clot|         41.40671|          2.18592|Entire home/apt|  152|             3|               81| 2023-08-27|             0.65|                             9|              58|                   22|HUTB-004663|\n",
      "|171816|Rental unit in Ba...| 400154|          Mireia|         Sant Martí|             el Clot|         41.40681|          2.18553|Entire home/apt|  150|             3|               49| 2023-08-27|             0.39|                             9|              67|                   27|HUTB-004664|\n",
      "+------+--------------------+-------+----------------+-------------------+--------------------+-----------------+-----------------+---------------+-----+--------------+-----------------+-----------+-----------------+------------------------------+----------------+---------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"listings_barcelona.csv\", header=True, multiLine=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddc4c6582c6c6a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-20T08:29:52.212589Z",
     "start_time": "2023-10-20T08:29:52.076927Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18086"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a13a5",
   "metadata": {},
   "source": [
    "## Load wikipedia data\n",
    "\n",
    "Load file from: https://pageviews.wmcloud.org/topviews/?project=uk.wikipedia.org&platform=all-access&date=2023-09&excludes= in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0308f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+------+\n",
      "|                Page|Edits|Editors| Views|\n",
      "+--------------------+-----+-------+------+\n",
      "|Умєров Рустем Енв...|   54|     34|127352|\n",
      "|             Ukr.net|    2|      1| 97183|\n",
      "|             Україна|    8|      6| 94568|\n",
      "|Кадиров Рамзан Ах...|   15|      8| 86347|\n",
      "|    Нагірний Карабах|   32|     12| 79264|\n",
      "+--------------------+-----+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"work/topviews-2023_09.csv\", header=True, multiLine=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c80db87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af89a01",
   "metadata": {},
   "source": [
    "## Additional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6aac0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stopwords = True  # Use True or False\n",
    "use_custom_stopwords = False  # Use True or False\n",
    "latent_features = 20  # Dimension of features\n",
    "nearest = 3  # Number of nearest neighbors\n",
    "\n",
    "# # Register Spark to be used by joblib\n",
    "register_spark()\n",
    "\n",
    "def timeit(func):\n",
    "    def timed(*args, **kwargs):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(\"Time elapsed for \" + func.__name__ + \": \" + str(end - start))\n",
    "        return result\n",
    "\n",
    "    return timed\n",
    "\n",
    "def read_data(spark, data):\n",
    "    if data == \"barcelona\":\n",
    "        df = spark.read.csv(\"listings_barcelona.csv\", header=True, multiLine=True)\n",
    "    elif data == \"titles\":\n",
    "        df = spark.read.csv(\"work/topviews-2023_09.csv\", header=True, multiLine=True)\n",
    "        # add id column as row number as string\n",
    "        df = df.withColumn(\"id\", monotonically_increasing_id().cast(\"string\"))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data\")\n",
    "    return df\n",
    "\n",
    "def limit_data(df, limit=50): # -1 means no limit\n",
    "    if limit > 0:\n",
    "        df = df.limit(limit)\n",
    "    return df\n",
    "\n",
    "def get_features(df, input_col=\"name\", output_col=\"features\", type_features=\"tfidf\"):\n",
    "    tokenizer = Tokenizer(inputCol=input_col, outputCol=\"words\")\n",
    "    df = tokenizer.transform(df)\n",
    "\n",
    "    if use_stopwords:\n",
    "        if use_custom_stopwords:\n",
    "            remover = StopWordsRemover(stopWords=stop_list, inputCol=\"words\", outputCol=\"clean_tokens\")\n",
    "        else:\n",
    "            remover = StopWordsRemover(inputCol=\"words\", outputCol=\"clean_tokens\")\n",
    "        df = remover.transform(df)\n",
    "        df = df.drop(\"words\")\n",
    "        df = df.withColumnRenamed(\"clean_tokens\", \"words\")\n",
    "\n",
    "    if type_features == \"tfidf\":\n",
    "        hashing = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=latent_features)\n",
    "        df = hashing.transform(df)\n",
    "\n",
    "        idf = IDF(inputCol=\"hash\", outputCol=output_col)\n",
    "        model = idf.fit(df)\n",
    "        df = model.transform(df)\n",
    "    elif type_features == \"word_to_vec\":\n",
    "        word_vec = Word2Vec(vectorSize=latent_features, minCount=0, inputCol=\"words\", outputCol=output_col)\n",
    "        model = word_vec.fit(df)\n",
    "        df = model.transform(df)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature \" + type_features)\n",
    "\n",
    "    return df\n",
    "\n",
    "@timeit\n",
    "def compute_gt(ds, spark, k=nearest, input_col=\"features\", output_col=\"gt_neighbors\"):\n",
    "    df = ds.toPandas()\n",
    "    features = df[input_col].tolist()\n",
    "    model = NearestNeighbors(n_neighbors=k + 1, algorithm='ball_tree').fit(features)\n",
    "    _, indices = model.kneighbors(features)\n",
    "    # remove self from neighbors\n",
    "    indices = [Vectors.dense(df[\"id\"][np.delete(ind, np.where(ind == i))].values) for i, ind in enumerate(indices)]\n",
    "    df[output_col] = indices\n",
    "\n",
    "    return spark.createDataFrame(df)\n",
    "\n",
    "@timeit\n",
    "def lsh_prediction(ds, spark, k=nearest, input_col=\"features\",\n",
    "                   output_col=\"ann_neighbors\", num_hash_tables=3):\n",
    "    model = MinHashLSH(inputCol=input_col, outputCol=output_col, numHashTables=num_hash_tables)\n",
    "    model = model.fit(ds)\n",
    "\n",
    "    # TODO There should be something better than this\n",
    "    pred = []\n",
    "    for i in tqdm(ds.collect()):\n",
    "        id_ = i[\"id\"]\n",
    "        key = i[input_col]\n",
    "        pred.append(model.approxNearestNeighbors(ds, key, k + 1).filter(col(\"id\") != id_).select(\"id\").collect())\n",
    "\n",
    "    pred = [Vectors.dense([i[\"id\"] for i in ann]) for ann in pred]\n",
    "    df = ds.toPandas()\n",
    "    df[output_col] = pred\n",
    "    ds = spark.createDataFrame(df)\n",
    "\n",
    "    return ds\n",
    "\n",
    "@timeit\n",
    "def grid_search_lsh(ds, spark, k=nearest, input_col=\"features\", output_col=\"ann_neighbors\"):\n",
    "    param_grid = [5, 10, 20, 100]\n",
    "    results = []\n",
    "    print(\"=\"*50)\n",
    "    print(\"Grid search for LSH\")\n",
    "\n",
    "    # Run the grid search in parallel\n",
    "    with Parallel(n_jobs=-1, backend=\"spark\") as parallel:\n",
    "        for num_hash_table in param_grid:\n",
    "            ds_ = lsh_prediction(ds, spark, k=k, input_col=input_col, output_col=output_col,\n",
    "                                num_hash_tables=num_hash_table)\n",
    "            acc = evaluation(ds_)\n",
    "            print(f\"Method: LSH - Num Hash Tables: {num_hash_table} - Accuracy: {acc}\\n\")\n",
    "            results.append((num_hash_table, acc))\n",
    "    print(\"=\"*50, \"\\n\\n\")\n",
    "    return results\n",
    "\n",
    "@timeit\n",
    "def annoy_prediction(ds, k=nearest, input_col=\"features\", output_col=\"ann_neighbors\", metric='angular', tree=10):\n",
    "    df = ds.toPandas()\n",
    "    features = df[input_col].tolist()\n",
    "    f = len(features[0])\n",
    "    t = AnnoyIndex(f, metric=metric)\n",
    "    for i, v in enumerate(features):\n",
    "        t.add_item(i, v)\n",
    "    t.build(tree)\n",
    "    pred = []\n",
    "    for i in tqdm(features):\n",
    "        pred.append(t.get_nns_by_vector(i, k + 1, include_distances=False)[1:])\n",
    "    pred = [Vectors.dense([df[\"id\"][i] for i in ann]) for ann in pred]\n",
    "    df[output_col] = pred\n",
    "    ds = spark.createDataFrame(df)\n",
    "\n",
    "    return ds\n",
    "\n",
    "@timeit\n",
    "def grid_search_annoy(ds, k=nearest, input_col=\"features\", output_col=\"ann_neighbors\"):\n",
    "    metrics = ['angular', 'euclidean', 'dot']\n",
    "    trees = [10, 100, 1000]\n",
    "    param_grid = [(metric, tree) for metric in metrics for tree in trees]\n",
    "    results = []\n",
    "    print(\"=\"*50)\n",
    "    print(\"Grid search for Annoy\")\n",
    "\n",
    "    # Run the grid search in parallel\n",
    "    with Parallel(n_jobs=-1, backend=\"spark\") as parallel:\n",
    "        for metric, tree in param_grid:\n",
    "            ds = annoy_prediction(ds, k=k, input_col=input_col, output_col=output_col, metric=metric, tree=tree)\n",
    "            acc = evaluation(ds)\n",
    "            print(f\"Method: Annoy - Metric: {metric} - Tree: {tree} - Accuracy: {acc}\\n\")\n",
    "            results.append((metric, tree, acc))\n",
    "    print(\"=\"*50, \"\\n\\n\")\n",
    "    return results\n",
    "\n",
    "def evaluation(ds):\n",
    "    acc = 0\n",
    "    for i in ds.collect():\n",
    "        gt = i[\"gt_neighbors\"]\n",
    "        ann = i[\"ann_neighbors\"]\n",
    "        gt.sort(), ann.sort()\n",
    "        acc += len(set(gt).intersection(set(ann)))\n",
    "    acc /= len(ds.collect()) * nearest\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31dafb",
   "metadata": {},
   "source": [
    "## Barcelona dataset\n",
    "### type_features: `Tf-idf`\n",
    "**Try LSH method from Pyspark(not optimized) and LSH method from Annoy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7cacb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = -1  # Use -1 for no limit\n",
    "data = \"barcelona\"  # Use \"barcelona\" or \"titles\"\n",
    "nearest = 3\n",
    "use_stopwords = True  # Use True or False\n",
    "use_custom_stopwords = False  # Use True or False\n",
    "latent_features = 20  # Dimension of features\n",
    "type_features = \"tfidf\"  # Use \"tfidf\" or \"word_to_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c638a273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for barcelona with limit -1 and features tfidf ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 12.146341562271118\n",
      "==================================================\n",
      "Grid search for LSH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [1:10:02<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 4207.41507434845\n",
      "Method: LSH - Num Hash Tables: 5 - Accuracy: 0.43508791330310737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [1:13:11<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 4396.357168674469\n",
      "Method: LSH - Num Hash Tables: 10 - Accuracy: 0.4507353754285082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [1:22:12<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 4937.006164073944\n",
      "Method: LSH - Num Hash Tables: 20 - Accuracy: 0.45020089203435437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [2:38:07<00:00,  1.91it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 9491.71913075447\n",
      "Method: LSH - Num Hash Tables: 100 - Accuracy: 0.45020089203435437\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_lsh: 23044.375668525696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.43508791330310737),\n",
       " (10, 0.4507353754285082),\n",
       " (20, 0.45020089203435437),\n",
       " (100, 0.45020089203435437)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, type_features=type_features)\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_lsh(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3264edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for barcelona with limit -1 and features tfidf ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 12.171573162078857\n",
      "==================================================\n",
      "Grid search for Annoy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:01<00:00, 12477.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 6.527449369430542\n",
      "Method: Annoy - Metric: angular - Tree: 10 - Accuracy: 0.2487743742858196\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:03<00:00, 5752.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 8.726189613342285\n",
      "Method: Annoy - Metric: angular - Tree: 100 - Accuracy: 0.657967488665266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:23<00:00, 767.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 38.11282658576965\n",
      "Method: Annoy - Metric: angular - Tree: 1000 - Accuracy: 0.9122525710494305\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:01<00:00, 12355.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 6.4307005405426025\n",
      "Method: Annoy - Metric: euclidean - Tree: 10 - Accuracy: 0.49808323196579307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:03<00:00, 5604.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 8.46422290802002\n",
      "Method: Annoy - Metric: euclidean - Tree: 100 - Accuracy: 0.7797928416086107\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:25<00:00, 712.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 33.91969347000122\n",
      "Method: Annoy - Metric: euclidean - Tree: 1000 - Accuracy: 0.9194588816395739\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:01<00:00, 12143.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 6.211334943771362\n",
      "Method: Annoy - Metric: dot - Tree: 10 - Accuracy: 0.016274097828891592\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:03<00:00, 5468.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 8.755682229995728\n",
      "Method: Annoy - Metric: dot - Tree: 100 - Accuracy: 0.028898964208043054\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:25<00:00, 720.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 36.909693002700806\n",
      "Method: Annoy - Metric: dot - Tree: 1000 - Accuracy: 0.016734859375575954\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_annoy: 181.61139702796936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('angular', 10, 0.2487743742858196),\n",
       " ('angular', 100, 0.657967488665266),\n",
       " ('angular', 1000, 0.9122525710494305),\n",
       " ('euclidean', 10, 0.49808323196579307),\n",
       " ('euclidean', 100, 0.7797928416086107),\n",
       " ('euclidean', 1000, 0.9194588816395739),\n",
       " ('dot', 10, 0.016274097828891592),\n",
       " ('dot', 100, 0.028898964208043054),\n",
       " ('dot', 1000, 0.016734859375575954)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, type_features=type_features)\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_annoy(df, k=nearest, input_col=\"features\", output_col=\"ann_neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743cfc7c",
   "metadata": {},
   "source": [
    "## Barcelona dataset\n",
    "### type_features: `Word2vec`\n",
    "**Try LSH method from Pyspark(not optimized) and LSH method from Annoy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0832263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = -1  # Use -1 for no limit\n",
    "data = \"barcelona\"  # Use \"barcelona\" or \"titles\"\n",
    "nearest = 3\n",
    "use_stopwords = True  # Use True or False\n",
    "use_custom_stopwords = False  # Use True or False\n",
    "latent_features = 20  # Dimension of features\n",
    "type_features = \"word_to_vec\"  # Use \"tfidf\" or \"word_to_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9448810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for barcelona with limit -1 and features word_to_vec ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 5.767719984054565\n",
      "==================================================\n",
      "Grid search for LSH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [1:10:09<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 4212.666715621948\n",
      "Method: LSH - Num Hash Tables: 5 - Accuracy: 7.372184746949758e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [1:23:23<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 5007.108249425888\n",
      "Method: LSH - Num Hash Tables: 10 - Accuracy: 7.372184746949758e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [1:34:53<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 5696.771778583527\n",
      "Method: LSH - Num Hash Tables: 20 - Accuracy: 7.372184746949758e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [3:44:11<00:00,  1.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 13454.690614700317\n",
      "Method: LSH - Num Hash Tables: 100 - Accuracy: 7.372184746949758e-05\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_lsh: 28378.0657453537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 7.372184746949758e-05),\n",
       " (10, 7.372184746949758e-05),\n",
       " (20, 7.372184746949758e-05),\n",
       " (100, 7.372184746949758e-05)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, type_features=type_features)\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_lsh(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2ad463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for barcelona with limit -1 and features word_to_vec ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 5.637991189956665\n",
      "==================================================\n",
      "Grid search for Annoy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:00<00:00, 68194.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 2.9605634212493896\n",
      "Method: Annoy - Metric: angular - Tree: 10 - Accuracy: 0.5126248663791515\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:02<00:00, 8610.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 5.4291160106658936\n",
      "Method: Annoy - Metric: angular - Tree: 100 - Accuracy: 0.6097718308820819\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:26<00:00, 690.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 35.24356245994568\n",
      "Method: Annoy - Metric: angular - Tree: 1000 - Accuracy: 0.621346160934793\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:00<00:00, 88690.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 2.9323172569274902\n",
      "Method: Annoy - Metric: euclidean - Tree: 10 - Accuracy: 0.5535957831103248\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:02<00:00, 8967.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 4.9190514087677\n",
      "Method: Annoy - Metric: euclidean - Tree: 100 - Accuracy: 0.6293081204614988\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:25<00:00, 705.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 31.750924587249756\n",
      "Method: Annoy - Metric: euclidean - Tree: 1000 - Accuracy: 0.6516642707066239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:00<00:00, 111019.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 2.9970247745513916\n",
      "Method: Annoy - Metric: dot - Tree: 10 - Accuracy: 0.001585019720594198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:01<00:00, 13459.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 4.712590217590332\n",
      "Method: Annoy - Metric: dot - Tree: 100 - Accuracy: 0.0058055954882229345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18086/18086 [00:17<00:00, 1035.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 27.277600049972534\n",
      "Method: Annoy - Metric: dot - Tree: 1000 - Accuracy: 0.0029120129750451547\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_annoy: 134.33812046051025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('angular', 10, 0.5126248663791515),\n",
       " ('angular', 100, 0.6097718308820819),\n",
       " ('angular', 1000, 0.621346160934793),\n",
       " ('euclidean', 10, 0.5535957831103248),\n",
       " ('euclidean', 100, 0.6293081204614988),\n",
       " ('euclidean', 1000, 0.6516642707066239),\n",
       " ('dot', 10, 0.001585019720594198),\n",
       " ('dot', 100, 0.0058055954882229345),\n",
       " ('dot', 1000, 0.0029120129750451547)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, type_features=type_features)\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_annoy(df, k=nearest, input_col=\"features\", output_col=\"ann_neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8216114c",
   "metadata": {},
   "source": [
    "## Wikipedia dataset\n",
    "### type_features: `Tf-idf`\n",
    "**Try LSH method from Pyspark(not optimized) and LSH method from Annoy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d7aaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "590efc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = -1  # Use -1 for no limit\n",
    "data = \"titles\"  # Use \"barcelona\" or \"titles\"\n",
    "nearest = 3\n",
    "use_stopwords = True  # Use True or False\n",
    "use_custom_stopwords = False  # Use True or False\n",
    "latent_features = 100  # Dimension of features\n",
    "type_features = \"tfidf\"  # Use \"tfidf\" or \"word_to_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0d760c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for titles with limit -1 and features tfidf ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 1.0458683967590332\n",
      "==================================================\n",
      "Grid search for LSH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:23<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 144.47205090522766\n",
      "Method: LSH - Num Hash Tables: 5 - Accuracy: 0.5932659932659933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:38<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 159.42340731620789\n",
      "Method: LSH - Num Hash Tables: 10 - Accuracy: 0.5959595959595959\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:02<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 123.37339949607849\n",
      "Method: LSH - Num Hash Tables: 20 - Accuracy: 0.5969696969696969\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:21<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 142.37637209892273\n",
      "Method: LSH - Num Hash Tables: 100 - Accuracy: 0.5973063973063973\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_lsh: 570.9063160419464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.5932659932659933),\n",
       " (10, 0.5959595959595959),\n",
       " (20, 0.5969696969696969),\n",
       " (100, 0.5973063973063973)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, input_col=\"Page\", type_features=type_features)\n",
    "df = df.filter(~F.col('features').cast('string').contains('[]'))\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_lsh(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5755f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for titles with limit -1 and features tfidf ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 0.8998205661773682\n",
      "==================================================\n",
      "Grid search for Annoy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 3442.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.7412104606628418\n",
      "Method: Annoy - Metric: angular - Tree: 10 - Accuracy: 0.5616161616161616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 3317.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.9903769493103027\n",
      "Method: Annoy - Metric: angular - Tree: 100 - Accuracy: 0.807070707070707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 1566.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 1.4089419841766357\n",
      "Method: Annoy - Metric: angular - Tree: 1000 - Accuracy: 0.8245791245791246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 3819.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.777529239654541\n",
      "Method: Annoy - Metric: euclidean - Tree: 10 - Accuracy: 0.6047138047138048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 3200.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.814356803894043\n",
      "Method: Annoy - Metric: euclidean - Tree: 100 - Accuracy: 0.877104377104377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 1493.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 1.304070234298706\n",
      "Method: Annoy - Metric: euclidean - Tree: 1000 - Accuracy: 0.9097643097643098\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 3808.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.8231987953186035\n",
      "Method: Annoy - Metric: dot - Tree: 10 - Accuracy: 0.19932659932659932\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 3328.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.7676877975463867\n",
      "Method: Annoy - Metric: dot - Tree: 100 - Accuracy: 0.17777777777777778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 1567.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 1.5372910499572754\n",
      "Method: Annoy - Metric: dot - Tree: 1000 - Accuracy: 0.1750841750841751\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_annoy: 11.711737632751465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('angular', 10, 0.5616161616161616),\n",
       " ('angular', 100, 0.807070707070707),\n",
       " ('angular', 1000, 0.8245791245791246),\n",
       " ('euclidean', 10, 0.6047138047138048),\n",
       " ('euclidean', 100, 0.877104377104377),\n",
       " ('euclidean', 1000, 0.9097643097643098),\n",
       " ('dot', 10, 0.19932659932659932),\n",
       " ('dot', 100, 0.17777777777777778),\n",
       " ('dot', 1000, 0.1750841750841751)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, input_col=\"Page\", type_features=type_features)\n",
    "df = df.filter(~F.col('features').cast('string').contains('[]'))\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_annoy(df, k=nearest, input_col=\"features\", output_col=\"ann_neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4fd47",
   "metadata": {},
   "source": [
    "## Wikipedia dataset\n",
    "### type_features: `Word2Vec`\n",
    "**Try LSH method from Pyspark(not optimized) and LSH method from Annoy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "946eca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = -1  # Use -1 for no limit\n",
    "data = \"titles\"  # Use \"barcelona\" or \"titles\"\n",
    "nearest = 3\n",
    "use_stopwords = True  # Use True or False\n",
    "use_custom_stopwords = False  # Use True or False\n",
    "latent_features = 20  # Dimension of features\n",
    "type_features = \"word_to_vec\"  # Use \"tfidf\" or \"word_to_vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1afd4b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for titles with limit -1 and features word_to_vec ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 0.3333303928375244\n",
      "==================================================\n",
      "Grid search for LSH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:11<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 131.77543473243713\n",
      "Method: LSH - Num Hash Tables: 5 - Accuracy: 0.0030303030303030303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:27<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 147.77966213226318\n",
      "Method: LSH - Num Hash Tables: 10 - Accuracy: 0.0030303030303030303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:23<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 144.15844702720642\n",
      "Method: LSH - Num Hash Tables: 20 - Accuracy: 0.0030303030303030303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [02:32<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for lsh_prediction: 152.8714427947998\n",
      "Method: LSH - Num Hash Tables: 100 - Accuracy: 0.0030303030303030303\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_lsh: 578.0813789367676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.0030303030303030303),\n",
       " (10, 0.0030303030303030303),\n",
       " (20, 0.0030303030303030303),\n",
       " (100, 0.0030303030303030303)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, input_col=\"Page\", type_features=type_features)\n",
    "df = df.filter(~F.col('features').cast('string').contains('[]'))\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_lsh(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b4e5f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for titles with limit -1 and features word_to_vec ...\n",
      "\n",
      "Calculating gt neighbors nearest neighbors, could take a while...\n",
      "Time elapsed for compute_gt: 0.30584263801574707\n",
      "==================================================\n",
      "Grid search for Annoy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 98369.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.3739008903503418\n",
      "Method: Annoy - Metric: angular - Tree: 10 - Accuracy: 0.40606060606060607\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 14765.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.3154134750366211\n",
      "Method: Annoy - Metric: angular - Tree: 100 - Accuracy: 0.5404040404040404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:01<00:00, 954.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 1.6667759418487549\n",
      "Method: Annoy - Metric: angular - Tree: 1000 - Accuracy: 0.5414141414141415\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 91010.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.25493407249450684\n",
      "Method: Annoy - Metric: euclidean - Tree: 10 - Accuracy: 0.5276094276094276\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 19516.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.42592692375183105\n",
      "Method: Annoy - Metric: euclidean - Tree: 100 - Accuracy: 0.9693602693602693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 1674.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.9297065734863281\n",
      "Method: Annoy - Metric: euclidean - Tree: 1000 - Accuracy: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 96304.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.19839692115783691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Annoy - Metric: dot - Tree: 10 - Accuracy: 0.0861952861952862\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 15471.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 0.2469027042388916\n",
      "Method: Annoy - Metric: dot - Tree: 100 - Accuracy: 0.05858585858585859\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 1358.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for annoy_prediction: 1.3285729885101318\n",
      "Method: Annoy - Metric: dot - Tree: 1000 - Accuracy: 0.05387205387205387\n",
      "\n",
      "================================================== \n",
      "\n",
      "\n",
      "Time elapsed for grid_search_annoy: 8.422160387039185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('angular', 10, 0.40606060606060607),\n",
       " ('angular', 100, 0.5404040404040404),\n",
       " ('angular', 1000, 0.5414141414141415),\n",
       " ('euclidean', 10, 0.5276094276094276),\n",
       " ('euclidean', 100, 0.9693602693602693),\n",
       " ('euclidean', 1000, 1.0),\n",
       " ('dot', 10, 0.0861952861952862),\n",
       " ('dot', 100, 0.05858585858585859),\n",
       " ('dot', 1000, 0.05387205387205387)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Reading data for \" + data + \" with limit \" + str(limit) + \" and features \" + type_features + \" ...\\n\")\n",
    "\n",
    "df = read_data(spark, data)\n",
    "df = limit_data(df, limit)\n",
    "\n",
    "df = get_features(df, input_col=\"Page\", type_features=type_features)\n",
    "df = df.filter(~F.col('features').cast('string').contains('[]'))\n",
    "\n",
    "print(\"Calculating gt neighbors nearest neighbors, could take a while...\")\n",
    "df = compute_gt(df, spark)\n",
    "\n",
    "grid_search_annoy(df, k=nearest, input_col=\"features\", output_col=\"ann_neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa29da7",
   "metadata": {},
   "source": [
    "# Results and conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b2cbb",
   "metadata": {},
   "source": [
    "Your report should include:\n",
    "- The accuracy of your model with at least 4 combinations of parameters\n",
    "- The computation time spent in the parameter tuning procedure (specify also the characteristics of the machine(s) that you have used)\n",
    "- What happen if we tune the parameters using Airbnb and the run with those parameters in The Wikipedia Dataset? What is the difference in accuracy with the parameters tuned directly in Wikipedia data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be284e1d",
   "metadata": {},
   "source": [
    "1. We run GridSearch for 4 combinations of parameters for each dataset. The results are shown in the table below:\n",
    "\n",
    "| Dataset |Model Type|Feature Type |Parameters | Accuracy | Time |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Barcelona | MinHashLSH | Tf-idf | numHashTables=5 | 0.435 | 4207.415 |\n",
    "| Barcelona | MinHashLSH | Tf-idf | numHashTables=10 | 0.450 | 4396.357 |\n",
    "| Barcelona | MinHashLSH | Tf-idf | numHashTables=20 | 0.450 | 4937.006 |\n",
    "| Barcelona | MinHashLSH | Tf-idf | numHashTables=100 | 0.450 | 9491.719 |\n",
    "| Barcelona | Annoy | Tf-idf | metric=angular, n_trees=10 | 0.248 | 6.527 |\n",
    "| Barcelona | Annoy | Tf-idf | metric=angular, n_trees=100 | 0.657 | 8.726 |\n",
    "| Barcelona | Annoy | Tf-idf | metric=angular, n_trees=1000 | 0.912 | 38.112 |\n",
    "| Barcelona | Annoy | Tf-idf | metric=euclidean, n_trees=10 | 0.498 | 6.430 |\n",
    "| Barcelona | Annoy | Tf-idf | metric=euclidean, n_trees=100 | 0.779 | 8.464 |\n",
    "| `Barcelona` | `Annoy` | `Tf-idf` | `metric=euclidean, n_trees=1000` | `0.919` | `33.919` |\n",
    "| Barcelona | Annoy | Tf-idf | metric=dot, n_trees=10 | 0.016 | 6.211 |\n",
    "| Barcelona | Annoy | Tf-idf | metric=dot, n_trees=100 | 0.028 | 8.755 |\n",
    "| Barcelona | Annoy | Tf-idf | metric=dot, n_trees=1000 | 0.016 | 36.909 |\n",
    "| Barcelona | MinHashLSH | Word2Vec | numHashTables=5 | 0.000 | 4212.667 |\n",
    "| Barcelona | MinHashLSH | Word2Vec | numHashTables=10 | 0.000 | 5007.108 |\n",
    "| Barcelona | MinHashLSH | Word2Vec | numHashTables=20 | 0.000 | 5696.772 |\n",
    "| Barcelona | MinHashLSH | Word2Vec | numHashTables=100 | 0.000 | 13454.691 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=angular, n_trees=10 | 0.512 | 2.961 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=angular, n_trees=100 | 0.610 | 5.429 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=angular, n_trees=1000 | 0.621 | 35.244 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=euclidean, n_trees=10 | 0.554 | 2.932 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=euclidean, n_trees=100 | 0.629 | 4.919 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=euclidean, n_trees=1000 | 0.652 | 31.751 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=dot, n_trees=10 | 0.002 | 2.997 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=dot, n_trees=100 | 0.006 | 4.713 |\n",
    "| Barcelona | Annoy | Word2Vec | metric=dot, n_trees=1000 | 0.003 | 27.278 |\n",
    "| Wikipedia | MinHashLSH | Tf-idf | numHashTables=5 | 0.593 | 144.472 |\n",
    "| Wikipedia | MinHashLSH | Tf-idf | numHashTables=10 | 0.596 | 159.423 |\n",
    "| Wikipedia | MinHashLSH | Tf-idf | numHashTables=20 | 0.596 | 123.373 |\n",
    "| Wikipedia | MinHashLSH | Tf-idf | numHashTables=100 | 0.597 | 142.376 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=angular, n_trees=10 | 0.561 | 0.741 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=angular, n_trees=100 | 0.807 | 0.990 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=angular, n_trees=1000 | 0.825 | 1.409 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=euclidean, n_trees=10 | 0.605 | 0.778 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=euclidean, n_trees=100 | 0.877 | 0.814 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=euclidean, n_trees=1000 | 0.910 | 1.304 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=dot, n_trees=10 | 0.199 | 0.823 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=dot, n_trees=100 | 0.178 | 0.768 |\n",
    "| Wikipedia | Annoy | Tf-idf | metric=dot, n_trees=1000 | 0.175 | 1.537 |\n",
    "| Wikipedia | MinHashLSH | Word2Vec | numHashTables=5 | 0.003 | 131.775 |\n",
    "| Wikipedia | MinHashLSH | Word2Vec | numHashTables=10 | 0.003 | 147.779 |\n",
    "| Wikipedia | MinHashLSH | Word2Vec | numHashTables=20 | 0.003 | 144.158 |\n",
    "| Wikipedia | MinHashLSH | Word2Vec | numHashTables=100 | 0.003 | 152.871 |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=angular, n_trees=10 | 0.406 | 0.374 |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=angular, n_trees=100 | 0.540 | 0.316 |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=angular, n_trees=1000 | 0.541 | 1.667 |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=euclidean, n_trees=10 | 0.527 | 0.255 |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=euclidean, n_trees=100 | 0.969 | 0.426 |\n",
    "| `Wikipedia` | `Annoy` | `Word2Vec` | `metric=euclidean, n_trees=1000` | `1.000` | `0.930` |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=dot, n_trees=10 | 0.086 | 0.198 |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=dot, n_trees=100 | 0.058 | 0.247 |\n",
    "| Wikipedia | Annoy | Word2Vec | metric=dot, n_trees=1000 | 0.053 | 1.329 |\n",
    "\n",
    "\n",
    "**Run Machine Type:** `macbook pro m1 16gb ram`, but we run in `docker container with ram limit 4gb and 4 cores`\n",
    "\n",
    "Also, we implemented both algorithms in MinHashLSH(but not optimized version) and Annoy Index( LSH method) and we test it for both possible type of vectores `Tf-idf` and `Word2Vec`.</br></br></br>\n",
    "\n",
    "\n",
    "\n",
    "2. What happen if we tune the parameters using Airbnb and the run with those parameters in The Wikipedia Dataset? What is the difference in accuracy with the parameters tuned directly in Wikipedia data?\n",
    "\n",
    "The optimal solution for Wikipedia data is Annoy with features Word2Vec and parameters: `metric=euclidean, n_trees=1000` and accuracy is `1.000`</br>\n",
    "The optimal solution for Barcelona data is Annoy with features Tf-Idf and parameters: `metric=euclidean, n_trees=1000` and accuracy is `0.919`</br>\n",
    "\n",
    "If we will use optimal parameters from Barcelona data for Wikipedia data, we will get accuracy `0.910`, which mean that we will lose `0.090` accuracy. It happened because for different data different optimal parameters needed and we cannot used one stack of parameters for all data. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
