{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lYBiMPuHVWc"
   },
   "source": [
    "### Project MMDB\n",
    "Authors:\n",
    "- Nazarii Drushchak\n",
    "- Igor Babin\n",
    "- Uliana Zbezhkhovska\n",
    "\n",
    "Task:\n",
    "\n",
    "- Consider all the changes done in the wikipedia as stream.\n",
    "    - Check here: https://wikitech.wikimedia.org/wiki/RCStream\n",
    "- Each action is received in json format.\n",
    "- Data is full of bots. There is a flag were programmers can indicate that an actions has been done by a bot.\n",
    "- Using this information as ground truth, develop a system able to classify users as bot or human.\n",
    "- Constrain: You need to sample, and just use the 20% of the data stream.\n",
    "- Describe the distribution of edits per users and bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPCi1BB5HdaT",
    "outputId": "d32d729f-ed44-49b3-e621-e76d15b28210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hVAVCT5oHBee",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:34:22.703711Z",
     "start_time": "2023-11-19T18:34:22.673743Z"
    }
   },
   "outputs": [],
   "source": [
    "from sseclient import SSEClient as EventSource\n",
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rai8F3O4LLQe",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:35:25.848146Z",
     "start_time": "2023-11-19T18:35:25.841887Z"
    }
   },
   "outputs": [],
   "source": [
    "LOAD_DATA=True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train data collection. We collected 800 thousand edits events from wikipedia. Each event is a json object. We store slices of 50k events in separate files for faster loading and processing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pSxIFNYsLRk4",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:35:30.480288Z",
     "start_time": "2023-11-19T18:35:30.478109Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "  import datetime\n",
    "  EVENTS = 10e6  # 1 million\n",
    "  SAVE_FREQ = 50000\n",
    "  URL = 'https://stream.wikimedia.org/v2/stream/recentchange'\n",
    "\n",
    "  count = 0\n",
    "  data = []\n",
    "\n",
    "  for event in EventSource(URL):\n",
    "    if event.event == 'message':\n",
    "        try:\n",
    "            change = json.loads(event.data)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        data.append(change)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if count >= EVENTS:\n",
    "            break\n",
    "\n",
    "        if count % SAVE_FREQ == 0:\n",
    "            with open(f'data_{count}.json', 'w') as outfile:\n",
    "                json.dump(data, outfile)\n",
    "                data = []\n",
    "            print('Time: {}, saved {} events'.format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For faster evaluation we store data in google drive and load it from there."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4Xq77cKoITF4",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:35:32.242272Z",
     "start_time": "2023-11-19T18:35:32.237597Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_from_drive():\n",
    "  !gdown 1JpYwYB1FsjUUOfZxCcr6wxdxt6Ap7PZ0\n",
    "  !gdown 18UCs2o_QRszZM_1M1OiXGTqywrjUL8hH\n",
    "  !gdown 1o1suE-eS8iUL1YUWgBhgkcjdPWEAlDQE\n",
    "  !gdown 1Wa-NQ-X4SCn3bMEbxNKGxsFpS7lc7yVm\n",
    "  !gdown 1OOx4S1EhsqxJ5wMAZsOU6iERJqsyYlur\n",
    "  !gdown 1l_18qAjkDf1fz7jZ-W5JQkj-sr71erAe\n",
    "  !gdown 1tjpl-RpxLWTkQ-v8P-eqS0IscW0qKX1h\n",
    "  !gdown 1pytAK5dY3Nd7GIqj0xfXkvW7kSY64tja\n",
    "  !gdown 1qp3_RM8m35kWaJnm0I8LgllqqjpRevTa\n",
    "  !gdown 1EHf8Focau2JlH0K874Wu7qrVMyKUHm4c\n",
    "  !gdown 1oaS07sIGdXkRNpXry0MoK32GMzNeSVCi\n",
    "  !gdown 1FGORShD9TkQGZOZxv0CrdipbxkHBbWD9\n",
    "  !gdown 11xgv0gHi9qB95aw525bEM8KLrpvR9o6P\n",
    "  !gdown 1SQZmTxFykknN7zypKbV8vR_MALBpeJlv\n",
    "  !gdown 166v5f1XKl5AS4wSd_RQxzWQN699a2xe2\n",
    "  !gdown 1Pr_Kwl6VfivIhfx9FojEdslAzzMu40nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VUhC_Qz9Lp8q"
   },
   "outputs": [],
   "source": [
    "if LOAD_DATA:\n",
    "  load_from_drive()\n",
    "else:\n",
    "  collect_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7y19NSlHHYmi",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:36:14.976094Z",
     "start_time": "2023-11-19T18:36:14.931069Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = [\"data_50000.json\", \"data_100000.json\", \"data_150000.json\", \"data_200000.json\", \"data_250000.json\",\n",
    "              \"data_300000.json\", \"data_350000.json\", \"data_400000.json\", \"data_450000.json\", \"data_500000.json\",\n",
    "              \"data_550000.json\", \"data_600000.json\", \"data_650000.json\", \"data_700000.json\", \"data_750000.json\"]\n",
    "val_path = [\"data_800000.json\"]\n",
    "\n",
    "filter_size = 10000\n",
    "filter_sizes = [1000, 10000, 15000]\n",
    "hash_functions = 20\n",
    "save_bot = True\n",
    "filter_type = 'user'  # 'user' or 'random\n",
    "filter_prob = 0.2\n",
    "target_amount = 500\n",
    "log_freq = 100\n",
    "URL = 'https://stream.wikimedia.org/v2/stream/recentchange'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to load data from files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SbWqNf0aHxik",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:36:16.722519Z",
     "start_time": "2023-11-19T18:36:16.714270Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(paths):\n",
    "    data = []\n",
    "    for p in paths:\n",
    "        with open(p, 'r') as f:\n",
    "            data += json.load(f)\n",
    "    return [{'bot': d['bot'], 'user': d['user']} for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First function in implementation of bloom filter. We use random hash functions to hash user names to filter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ow9rN1wBJ1uz",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:36:17.043006Z",
     "start_time": "2023-11-19T18:36:17.040772Z"
    }
   },
   "outputs": [],
   "source": [
    "def rand_hash(modulus, a=None, b=None):\n",
    "    if a is None or b is None:\n",
    "        a, b = random.randint(1, modulus - 1), random.randint(1, modulus - 1)\n",
    "    # print(a, b)\n",
    "    func = lambda x: hash(x) % (a + b) % modulus\n",
    "    return {'func': func, 'a': a, 'b': b}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just a wrapper to generate multiple random hash functions together with ability to load pretrained hash functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o5sF-23pJ3CL",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:36:17.427648Z",
     "start_time": "2023-11-19T18:36:17.423096Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_hashes(modulus, amount_hash_functions, seed=42, params=None):\n",
    "    # e.x params = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\n",
    "    random.seed(seed)\n",
    "    if params is None:\n",
    "        fns = [rand_hash(modulus) for _ in range(amount_hash_functions)]\n",
    "    else:\n",
    "        fns = [rand_hash(modulus, **param) for param in params]\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Util function to check if user is in filter based on hash functions. If all hash functions return 1, then user is in filter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "1FBFNaLpJ4fX",
    "ExecuteTime": {
     "end_time": "2023-11-19T19:07:27.364854Z",
     "start_time": "2023-11-19T19:07:27.364468Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_str_in_filter(bloom_filter, hashes, data):\n",
    "    for h in hashes:\n",
    "        if bloom_filter[h['func'](data[\"user\"])] == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to evaluate filter on data. We use ground truth to compare with predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WYjEXW0GJ5yU",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:36:20.719344Z",
     "start_time": "2023-11-19T18:36:20.711383Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_filter(bloom_filter, hashes, data):\n",
    "    gt = [d['bot'] for d in data]\n",
    "    pred = [is_str_in_filter(bloom_filter, hashes, d) for d in data]\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to fit filter on data. We use pre-defined hash functions to fit filter. We set 1 in bloom filter where hash function it's index in array. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Rb26xmQ_J69I",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:36:21.174502Z",
     "start_time": "2023-11-19T18:36:21.168106Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_filter(bloom_filter, hashes, data, save_bot=save_bot):\n",
    "    for d in data:\n",
    "        if d['bot'] != 1 and save_bot:\n",
    "            continue\n",
    "        for h in hashes:\n",
    "            bloom_filter[h['func'](d['user'])] = 1\n",
    "\n",
    "    return bloom_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just accuracy function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PrbEz6bYJ8H_",
    "ExecuteTime": {
     "end_time": "2023-11-19T18:36:21.963020Z",
     "start_time": "2023-11-19T18:36:21.955993Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(gt, pred):\n",
    "    return sum([1 for i in range(len(gt)) if gt[i] == pred[i]]) / len(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to get best filter based on accuracy on train and val data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LDUl9632J9dZ",
    "ExecuteTime": {
     "end_time": "2023-11-19T19:09:38.996087Z",
     "start_time": "2023-11-19T19:09:38.986031Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_filter():\n",
    "    train_data = load_data(train_path)\n",
    "    val_data = load_data(val_path)\n",
    "\n",
    "    total_train = len(train_data)\n",
    "    amount_bots_train = len([d for d in train_data if d['bot'] == 1])\n",
    "\n",
    "    total_val = len(val_data)\n",
    "    amount_bots_val = len([d for d in val_data if d['bot'] == 1])\n",
    "\n",
    "    print(\"Total train amount of users: \", total_train)\n",
    "    print(\"Total val amount of users: \", total_val)\n",
    "\n",
    "    print(\"Train amount of bots: \", amount_bots_train)\n",
    "    print(\"Val amount of bots: \", amount_bots_val)\n",
    "\n",
    "    # Calculate amount of the same bots from val in train\n",
    "    names = set([d['user'] for d in train_data])\n",
    "    same_users = 0\n",
    "    same_bots = 0\n",
    "    for d in val_data:\n",
    "        if d[\"user\"] in names:\n",
    "            if d['bot'] == 0:\n",
    "                same_users += 1\n",
    "            else:\n",
    "                same_bots += 1\n",
    "    print(\"Amount of the same users from val in train: \", same_users)\n",
    "    print(\"Amount of the same bots from val in train: \", same_bots)\n",
    "\n",
    "    # with open('bloom_filter.json', 'r') as f:\n",
    "    #     data = json.load(f)\n",
    "    # bloom_filter_saved = data['filter']\n",
    "    # hashes_a_b = data['hashes']\n",
    "\n",
    "    best_acc = -1\n",
    "    for filter_size in filter_sizes:\n",
    "        bloom_filter = [0] * filter_size\n",
    "        hashes = random_hashes(filter_size, hash_functions)#, params=hashes_a_b)\n",
    "\n",
    "        bloom_filter_fitted = fit_filter(bloom_filter, hashes, train_data)\n",
    "\n",
    "        gt, pred = eval_filter(bloom_filter_fitted, hashes, train_data)\n",
    "        print(\"Train accuracy: \", round(accuracy(gt, pred), 3))\n",
    "\n",
    "        gt, pred = eval_filter(bloom_filter_fitted, hashes, val_data)\n",
    "        eval_acc = round(accuracy(gt, pred), 3)\n",
    "        print(\"Val accuracy: \", eval_acc)\n",
    "\n",
    "        if eval_acc > best_acc:\n",
    "            best_filter_size = filter_size\n",
    "            best_acc = eval_acc\n",
    "            best_filter = bloom_filter_fitted\n",
    "            best_hashes = hashes\n",
    "\n",
    "    # Save filter\n",
    "    # with open('bloom_filter.json', 'w') as f:\n",
    "    #     json.dump({'filter': bloom_filter_fitted, 'hashes': [{'a': h['a'], 'b': h['b']} for h in hashes]}, f)\n",
    "    print(f\"Best filter size is: {best_filter_size} \")\n",
    "    return best_filter, best_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMgfy0w9KEGq",
    "outputId": "dc486d53-2553-4c25-bdd0-271af8c2a123",
    "ExecuteTime": {
     "end_time": "2023-11-19T19:10:01.856370Z",
     "start_time": "2023-11-19T19:09:39.766292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train amount of users:  750000\n",
      "Total val amount of users:  50000\n",
      "Train amount of bots:  271831\n",
      "Val amount of bots:  15798\n",
      "Amount of the same users from val in train:  28251\n",
      "Amount of the same bots from val in train:  15775\n",
      "Train accuracy:  0.386\n",
      "Val accuracy:  0.343\n",
      "Train accuracy:  0.94\n",
      "Val accuracy:  0.941\n",
      "Train accuracy:  0.941\n",
      "Val accuracy:  0.943\n",
      "Best filter size is: 15000 \n"
     ]
    }
   ],
   "source": [
    "bloom_filter, hashes = get_filter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Logistic regression to confirm is user bot or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T19:03:28.471476Z",
     "start_time": "2023-11-19T19:03:28.464462Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to preprocess text using pretrained model. We use BERT model to get embeddings of user names and then use Logistic regression to classify user as bot or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:12:44.751034Z",
     "start_time": "2023-11-19T19:12:44.742959Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\n",
    "    model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5').to(device)\n",
    "\n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    if type(data['user']) == str:\n",
    "        encoded_input = tokenizer(data['user'], padding=True, truncation=True, return_tensors='pt')\n",
    "    else:\n",
    "        encoded_input = tokenizer(data['user'].values.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input.to(device))\n",
    "\n",
    "    # Perform pooling. In this case, mean pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return pd.DataFrame(sentence_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "train_data = load_data(train_path)[:5000]\n",
    "x_train = pd.DataFrame(train_data)\n",
    "y_train = x_train['bot'].astype('int')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T18:46:50.764238Z",
     "start_time": "2023-11-19T18:46:37.696287Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "x_train = preprocess_text(x_train)   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T18:49:51.980747Z",
     "start_time": "2023-11-19T18:46:55.666912Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train Logistic regression model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/engineer/anaconda/envs/igor-ucu/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='sag').fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T18:51:17.529298Z",
     "start_time": "2023-11-19T18:51:14.772911Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate model on validation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-19T19:12:47.012458Z"
    }
   },
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "val_data = load_data(val_path)[:5000] # use it from previous node\n",
    "x_val = pd.DataFrame(val_data)\n",
    "y_test = x_val['bot'].astype('int')\n",
    "y_pred = model.predict(preprocess_text(x_val))\n",
    "print(f'The accuracy of Logistic regression model is {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final function to predict if user is bot or not. We use bloom filter first to check if user is in filter. If bloom filter suggests that user is not bot, we use Logistic regression to confirm it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T18:56:04.555535Z",
     "start_time": "2023-11-19T18:56:04.546108Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_bot(data, bloom_filter, hashes):\n",
    "    # Check bloom filter first\n",
    "    bloom_filter_result = is_str_in_filter(bloom_filter, hashes, data)\n",
    "\n",
    "    # If bloom filter suggests not bot, use logistic regression for confirmation\n",
    "    if not bloom_filter_result:\n",
    "        print(\"Run logistic regression\")\n",
    "        logistic_regression_result = model.predict(preprocess_text(data))\n",
    "        if logistic_regression_result:\n",
    "            # store in bloom filter\n",
    "            bloom_filter = fit_filter(bloom_filter, hashes, [data])\n",
    "        return bloom_filter, logistic_regression_result\n",
    "\n",
    "    return bloom_filter, True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def left_user(user):\n",
    "    if filter_type == 'user':\n",
    "        return hash(user['user']) % 100 > filter_prob * 100\n",
    "    elif filter_type == 'random':\n",
    "        return random.random() % 100 > filter_prob * 100\n",
    "    else:\n",
    "        return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-19T19:02:37.704031Z",
     "start_time": "2023-11-19T19:01:20.283140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event:  1\n",
      "Event:  2\n",
      "Event:  3\n",
      "Event:  4\n",
      "Run logistic regression\n",
      "Event:  5\n",
      "Run logistic regression\n",
      "Processed 5 events\n",
      "Event:  6\n",
      "Run logistic regression\n",
      "Event:  7\n",
      "Run logistic regression\n",
      "Event:  8\n",
      "Event:  9\n",
      "Run logistic regression\n",
      "Event:  10\n",
      "Processed 10 events\n",
      "Event:  11\n",
      "Run logistic regression\n",
      "Event:  12\n",
      "Run logistic regression\n",
      "Event:  13\n",
      "Run logistic regression\n",
      "Event:  14\n",
      "Run logistic regression\n",
      "Event:  15\n",
      "Run logistic regression\n",
      "Processed 15 events\n",
      "Event:  16\n",
      "Run logistic regression\n",
      "Event:  17\n",
      "Run logistic regression\n",
      "Event:  18\n",
      "Run logistic regression\n",
      "Event:  19\n",
      "Run logistic regression\n",
      "Event:  20\n",
      "Run logistic regression\n",
      "Processed 20 events\n",
      "Event:  21\n",
      "Run logistic regression\n",
      "Event:  22\n",
      "Event:  23\n",
      "Run logistic regression\n",
      "Event:  24\n",
      "Run logistic regression\n",
      "Event:  25\n",
      "Processed 25 events\n",
      "Event:  26\n",
      "Event:  27\n",
      "Run logistic regression\n",
      "Event:  28\n",
      "Run logistic regression\n",
      "Event:  29\n",
      "Run logistic regression\n",
      "Event:  30\n",
      "Run logistic regression\n",
      "Processed 30 events\n",
      "Event:  31\n",
      "Run logistic regression\n",
      "Event:  32\n",
      "Run logistic regression\n",
      "Event:  33\n",
      "Event:  34\n",
      "Event:  35\n",
      "Processed 35 events\n",
      "Event:  36\n",
      "Run logistic regression\n",
      "Event:  37\n",
      "Run logistic regression\n",
      "Event:  38\n",
      "Run logistic regression\n",
      "Event:  39\n",
      "Run logistic regression\n",
      "Event:  40\n",
      "Processed 40 events\n",
      "Event:  41\n",
      "Run logistic regression\n",
      "Event:  42\n",
      "Event:  43\n",
      "Run logistic regression\n",
      "Event:  44\n",
      "Event:  45\n",
      "Run logistic regression\n",
      "Processed 45 events\n",
      "Event:  46\n",
      "Run logistic regression\n",
      "Event:  47\n",
      "Run logistic regression\n",
      "Event:  48\n",
      "Event:  49\n",
      "Event:  50\n",
      "Run logistic regression\n",
      "Processed 50 events\n",
      "Accuracy: 1.0\n",
      "Non-filtered amount of bots/users: 20/48\n",
      "Filtered amount of bots/users: 17/33\n",
      "Predicted amount of bots/users: [17]/[33]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "non_filtered_gt = []\n",
    "gt_bot = []\n",
    "pred_bot = []\n",
    "log_freq = 5\n",
    "target_amount = 50\n",
    "for event in EventSource(URL):\n",
    "    if event.event == 'message':\n",
    "        try:\n",
    "            change = json.loads(event.data)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        non_filtered_gt.append(change[\"bot\"])\n",
    "        if not left_user(change): \n",
    "            continue\n",
    "\n",
    "        count += 1\n",
    "        print(\"Event: \", count)\n",
    "\n",
    "        gt_bot.append(change['bot'])\n",
    "        bloom_filter, is_bot = predict_bot(change, bloom_filter, hashes)\n",
    "        pred_bot.append(is_bot)\n",
    "\n",
    "        if count % log_freq == 0:\n",
    "            print('Processed {} events'.format(count))\n",
    "\n",
    "        if count >= target_amount:\n",
    "            break\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy(gt_bot, pred_bot)))\n",
    "print('Non-filtered amount of bots/users: {}/{}'.format(sum(non_filtered_gt), len(non_filtered_gt) - sum(non_filtered_gt)))\n",
    "print('Filtered amount of bots/users: {}/{}'.format(sum(gt_bot), len(gt_bot) - sum(gt_bot)))\n",
    "print('Predicted amount of bots/users: {}/{}'.format(sum(pred_bot), len(pred_bot) - sum(pred_bot)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
